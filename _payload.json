[{"data":1,"prerenderedAt":1360},["ShallowReactive",2],{"blog":3},[4,256,899,1217],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":9,"description":10,"author":11,"gh_nickname":12,"image":13,"date":14,"body":15,"_type":250,"_id":251,"_source":252,"_file":253,"_stem":254,"_extension":255},"/blog/weekly_update_29-11","blog",false,"","Weekly Update 29-11","29-11 weekly update from the PlumJuice team","Simone Palmieri","sudo-err","https://res.cloudinary.com/dllutk9zl/image/upload/v1732897195/presentation_cut_lhrepv.jpg","2024-11-29",{"type":16,"children":17,"toc":242},"root",[18,27,33,46,65,91,108,114,138,152,158,170,184,190,209,215,227,237],{"type":19,"tag":20,"props":21,"children":23},"element","h2",{"id":22},"we-presented-the-cluster-to-students",[24],{"type":25,"value":26},"text","We presented the cluster to students!",{"type":19,"tag":28,"props":29,"children":30},"p",{},[31],{"type":25,"value":32},"This week was mainly focused on the class presentation we did on wednesday, so this one will be a lighter update with less HPC news and more... juice (I don't even know).",{"type":19,"tag":28,"props":34,"children":35},{},[36,38,44],{"type":25,"value":37},"First of all, our biggest thank you goes to professor ",{"type":19,"tag":39,"props":40,"children":41},"strong",{},[42],{"type":25,"value":43},"Daniele De Sensi",{"type":25,"value":45}," that allowed us to talk at his lecture on Multicore Programming.",{"type":19,"tag":28,"props":47,"children":48},{},[49,51,56,58,63],{"type":25,"value":50},"We were able to talk about the genesis, current state and future developments of the project, and a huge thanks goes to the ",{"type":19,"tag":39,"props":52,"children":53},{},[54],{"type":25,"value":55},"students",{"type":25,"value":57}," that were really passionate about the plum! Or maybe they just wanted ",{"type":19,"tag":39,"props":59,"children":60},{},[61],{"type":25,"value":62},"stickers",{"type":25,"value":64},". But I get it, they're amazing.",{"type":19,"tag":28,"props":66,"children":67},{},[68,70,81,83,89],{"type":25,"value":69},"Another lucky event took place at the presentation, but you'll have to check ",{"type":19,"tag":71,"props":72,"children":78},"a",{"href":73,"rel":74,"target":77},"https://instagram.com/plumjuiceteam",[75,76],"noopener","noreferer","_blank",[79],{"type":25,"value":80},"our Instagram",{"type":25,"value":82}," for that (that's what we call a ",{"type":19,"tag":84,"props":85,"children":86},"em",{},[87],{"type":25,"value":88},"\"call to action\"",{"type":25,"value":90},", you nerds).",{"type":19,"tag":92,"props":93,"children":94},"figure",{},[95,97,101,102],{"type":25,"value":96},"\n    ",{"type":19,"tag":98,"props":99,"children":100},"img",{"src":13,"alt":8},[],{"type":25,"value":96},{"type":19,"tag":103,"props":104,"children":105},"figcaption",{},[106],{"type":25,"value":107},"Presentation in classroom on November 27th",{"type":19,"tag":20,"props":109,"children":111},{"id":110},"were-on-patreon",[112],{"type":25,"value":113},"We're on Patreon!",{"type":19,"tag":28,"props":115,"children":116},{},[117,119,124,126,136],{"type":25,"value":118},"Yes you read that right (hopefully)! You can now support us on the ",{"type":19,"tag":84,"props":120,"children":121},{},[122],{"type":25,"value":123},"second",{"type":25,"value":125}," most famous platform to give money to perfect strangers! Just go ",{"type":19,"tag":39,"props":127,"children":128},{},[129],{"type":19,"tag":71,"props":130,"children":133},{"href":131,"rel":132,"target":77},"https://www.patreon.com/PlumJuiceTeam",[75,76],[134],{"type":25,"value":135},"HERE",{"type":25,"value":137}," and chek out the juicy rewards we made for our top supporters, you won't regret it.",{"type":19,"tag":92,"props":139,"children":140},{},[141,142,146,147],{"type":25,"value":96},{"type":19,"tag":98,"props":143,"children":145},{"src":144,"alt":8},"https://res.cloudinary.com/dllutk9zl/image/upload/v1732897197/patreon_yvnrh5.jpg",[],{"type":25,"value":96},{"type":19,"tag":103,"props":148,"children":149},{},[150],{"type":25,"value":151},"Go check it out!",{"type":19,"tag":20,"props":153,"children":155},{"id":154},"our-amazing-woodwork",[156],{"type":25,"value":157},"Our amazing woodwork",{"type":19,"tag":28,"props":159,"children":160},{},[161,163,168],{"type":25,"value":162},"You've already seen it empty in the previous weekly update, but if you were at the live presentation you had a glance of the immense computational power we assembled. It would be cruel to keep this magnificent view to ourselves, so here it is: ",{"type":19,"tag":39,"props":164,"children":165},{},[166],{"type":25,"value":167},"the wooden rack",{"type":25,"value":169},".",{"type":19,"tag":92,"props":171,"children":172},{},[173,174,178,179],{"type":25,"value":96},{"type":19,"tag":98,"props":175,"children":177},{"src":176,"alt":8},"https://res.cloudinary.com/dllutk9zl/image/upload/v1732897201/wood_rack_cut_df3rzh.jpg",[],{"type":25,"value":96},{"type":19,"tag":103,"props":180,"children":181},{},[182],{"type":25,"value":183},"Majestic.",{"type":19,"tag":20,"props":185,"children":187},{"id":186},"weve-got-99-problems-and-topology-is-one-but-not-for-long",[188],{"type":25,"value":189},"We've got 99 problems and topology is one (but not for long)",{"type":19,"tag":28,"props":191,"children":192},{},[193,195,200,202,207],{"type":25,"value":194},"If you're one of our beloved early plums you know that the ",{"type":19,"tag":39,"props":196,"children":197},{},[198],{"type":25,"value":199},"cluster's topology",{"type":25,"value":201}," is one of the main topics we spend time working on. At its current state it's not optimal, as some network congestion is possible due to the type of switches we use now and the DHCP state. It is not a problem for now, since the workload is light, but it may be in the near future. Luckily, a solution is in the air and it involves some ",{"type":19,"tag":84,"props":203,"children":204},{},[205],{"type":25,"value":206},"soldering",{"type":25,"value":208},", so if you happen to smell something burning don't worry it's us (or you're having a stroke).",{"type":19,"tag":20,"props":210,"children":212},{"id":211},"ernestos-challenge-of-the-week",[213],{"type":25,"value":214},"Ernesto's challenge of the week",{"type":19,"tag":28,"props":216,"children":217},{},[218,220,225],{"type":25,"value":219},"The world's best SysAdmin never rests, and this week he found another worthy opponent: ",{"type":19,"tag":39,"props":221,"children":222},{},[223],{"type":25,"value":224},"Docker swarm",{"type":25,"value":226},". Will he win? Will this new test get the better of him? Stay tuned to find out...",{"type":19,"tag":92,"props":228,"children":231},{"className":229},[230],"not-centerd",[232,233],{"type":25,"value":96},{"type":19,"tag":98,"props":234,"children":236},{"src":235,"alt":8},"https://res.cloudinary.com/dllutk9zl/image/upload/v1732897511/jojo_kdui48.jpg",[],{"type":19,"tag":238,"props":239,"children":241},"credits",{":authors":240},"[{\"name\":\"Simone Palmieri\",\"github_nickname\":\"sudo-err\"}]",[],{"title":8,"searchDepth":243,"depth":243,"links":244},2,[245,246,247,248,249],{"id":22,"depth":243,"text":26},{"id":110,"depth":243,"text":113},{"id":154,"depth":243,"text":157},{"id":186,"depth":243,"text":189},{"id":211,"depth":243,"text":214},"markdown","content:blog:weekly_update_29-11.md","content","blog/weekly_update_29-11.md","blog/weekly_update_29-11","md",{"_path":257,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":258,"description":259,"author":260,"gh_nickname":261,"date":262,"image":263,"body":264,"_type":250,"_id":896,"_source":252,"_file":897,"_stem":898,"_extension":255},"/blog/weekly_update_22-11","Weekly Update 22-11","The week began with the cluster almost in a ready state, but issues emerged during testing with MPI and SLURM. Specifically: SLURM (srun) Only ran in singleton mode without errors but failed to scale. MPI (mpirun) Crashed outright, returning an error: \"PSEC: munge failed to create credential: Socket communication error.\"","Alessio Olivieri","Alessio-Olivieri","2024-11-22T00:00:00.000Z","https://res.cloudinary.com/dllutk9zl/image/upload/v1732270953/Plum_at_SC24_eozc8d.jpg",{"type":16,"children":265,"toc":877},[266,272,277,318,345,352,364,369,385,390,428,434,453,472,478,487,492,499,505,510,516,522,527,533,570,576,581,587,592,625,631,636,642,830,836,855,860,873],{"type":19,"tag":20,"props":267,"children":269},{"id":268},"cluster-issues-with-mpi-and-slurm",[270],{"type":25,"value":271},"Cluster Issues with MPI and SLURM",{"type":19,"tag":28,"props":273,"children":274},{},[275],{"type":25,"value":276},"The week began with the cluster almost in a ready state, but issues emerged during testing with MPI and SLURM. Specifically:",{"type":19,"tag":278,"props":279,"children":280},"ul",{},[281,301],{"type":19,"tag":282,"props":283,"children":284},"li",{},[285,299],{"type":19,"tag":39,"props":286,"children":287},{},[288,290,297],{"type":25,"value":289},"SLURM (",{"type":19,"tag":291,"props":292,"children":294},"code",{"className":293},[],[295],{"type":25,"value":296},"srun",{"type":25,"value":298},")",{"type":25,"value":300},": Only ran in singleton mode without errors but failed to scale.",{"type":19,"tag":282,"props":302,"children":303},{},[304,316],{"type":19,"tag":39,"props":305,"children":306},{},[307,309,315],{"type":25,"value":308},"MPI (",{"type":19,"tag":291,"props":310,"children":312},{"className":311},[],[313],{"type":25,"value":314},"mpirun",{"type":25,"value":298},{"type":25,"value":317},": Crashed outright, returning an error: \"PSEC: munge failed to create credential: Socket communication error.\"",{"type":19,"tag":28,"props":319,"children":320},{},[321,323,328,330,336,338,343],{"type":25,"value":322},"The issue was ",{"type":19,"tag":39,"props":324,"children":325},{},[326],{"type":25,"value":327},"Munge",{"type":25,"value":329},", a tool SLURM uses for authentication across nodes. Although all nodes had it installed and shared the same ",{"type":19,"tag":291,"props":331,"children":333},{"className":332},[],[334],{"type":25,"value":335},"munge.key",{"type":25,"value":337},", ",{"type":19,"tag":291,"props":339,"children":341},{"className":340},[],[342],{"type":25,"value":314},{"type":25,"value":344}," continued to fail.",{"type":19,"tag":346,"props":347,"children":349},"h3",{"id":348},"solution-attempts-and-breakthrough",[350],{"type":25,"value":351},"Solution Attempts and Breakthrough",{"type":19,"tag":28,"props":353,"children":354},{},[355,357,362],{"type":25,"value":356},"After trying several fixes, we found a relevant article by ",{"type":19,"tag":39,"props":358,"children":359},{},[360],{"type":25,"value":361},"Hossein Ghorbanfekr",{"type":25,"value":363}," on Medium, which detailed a similar setup using Raspberry Pi 4 with a Pi 3 as master: he compiled SLURM, and MPI from scratch and his setup was working, however the same authentication issue persisted.",{"type":19,"tag":28,"props":365,"children":366},{},[367],{"type":25,"value":368},"Later, a deeper research persisted by Ernesto revealed an Intel cluster user‚Äôs solution:",{"type":19,"tag":278,"props":370,"children":371},{},[372],{"type":19,"tag":282,"props":373,"children":374},{},[375,377,383],{"type":25,"value":376},"By setting ",{"type":19,"tag":291,"props":378,"children":380},{"className":379},[],[381],{"type":25,"value":382},"PMIX_MCA_psec0=native",{"type":25,"value":384},", MPI bypassed Munge and used its default authentication. This change allowed MPI to communicate properly across nodes without the Munge authentication layer.",{"type":19,"tag":28,"props":386,"children":387},{},[388],{"type":25,"value":389},"Testing this:",{"type":19,"tag":278,"props":391,"children":392},{},[393,415],{"type":19,"tag":282,"props":394,"children":395},{},[396,405,407,413],{"type":19,"tag":39,"props":397,"children":398},{},[399],{"type":19,"tag":291,"props":400,"children":402},{"className":401},[],[403],{"type":25,"value":404},"PMIX_MCA_psec0=munge",{"type":25,"value":406}," with ",{"type":19,"tag":291,"props":408,"children":410},{"className":409},[],[411],{"type":25,"value":412},"srun -n 16 hostname",{"type":25,"value":414}," worked.",{"type":19,"tag":282,"props":416,"children":417},{},[418,426],{"type":19,"tag":39,"props":419,"children":420},{},[421],{"type":19,"tag":291,"props":422,"children":424},{"className":423},[],[425],{"type":25,"value":314},{"type":25,"value":427}," still faced issues, which we plan to address when we have more time, as the current priority is communication stability.",{"type":19,"tag":20,"props":429,"children":431},{"id":430},"hardware-upgrade",[432],{"type":25,"value":433},"Hardware Upgrade",{"type":19,"tag":28,"props":435,"children":436},{},[437,439,444,446,451],{"type":25,"value":438},"We decided to include the previous master ",{"type":19,"tag":39,"props":440,"children":441},{},[442],{"type":25,"value":443},"Ciz0",{"type":25,"value":445}," (The only node with 8GB of RAM) in the computational nodes and passed the role of the master to ",{"type":19,"tag":39,"props":447,"children":448},{},[449],{"type":25,"value":450},"Andrea",{"type":25,"value":452},", a Raspberry Pi 3b with 1 GB of RAM, enhancing the cluster specs:",{"type":19,"tag":278,"props":454,"children":455},{},[456,464],{"type":19,"tag":282,"props":457,"children":458},{},[459],{"type":19,"tag":39,"props":460,"children":461},{},[462],{"type":25,"value":463},"28GB -> 36 GB of RAM",{"type":19,"tag":282,"props":465,"children":466},{},[467],{"type":19,"tag":39,"props":468,"children":469},{},[470],{"type":25,"value":471},"28->32 cores",{"type":19,"tag":346,"props":473,"children":475},{"id":474},"potential-problems-of-this-approach",[476],{"type":25,"value":477},"Potential Problems of this approach",{"type":19,"tag":28,"props":479,"children":480},{},[481,485],{"type":19,"tag":39,"props":482,"children":483},{},[484],{"type":25,"value":443},{"type":25,"value":486}," still handles the filesystem... This might be a potential bottleneck when the multiple users try to use the server at the same time since it has more tasks to do.",{"type":19,"tag":28,"props":488,"children":489},{},[490],{"type":25,"value":491},"The problem will be solved when the filesystem will be handled by the master, which will happen when we get more raspberries",{"type":19,"tag":493,"props":494,"children":496},"h5",{"id":495},"saverio-is-now-satisfied-that-his-previously-underutilized-hardware-was-put-to-work",[497],{"type":25,"value":498},"Saverio is now satisfied that his previously underutilized hardware was put to work!",{"type":19,"tag":346,"props":500,"children":502},{"id":501},"next-steps",[503],{"type":25,"value":504},"Next Steps",{"type":19,"tag":28,"props":506,"children":507},{},[508],{"type":25,"value":509},"Using munge for the authentication will be scheduled for later, as we prioritize cluster stability for the presentation of the 13/11/24 (which has been rescheduled for the 20/11/24)",{"type":19,"tag":20,"props":511,"children":513},{"id":512},"plum-juice-cluster-presentation-in-the-multicore-class",[514],{"type":25,"value":515},"Plum juice cluster presentation in the multicore class",{"type":19,"tag":346,"props":517,"children":519},{"id":518},"make-the-world-know-about-us",[520],{"type":25,"value":521},"Make the world know about us.",{"type":19,"tag":28,"props":523,"children":524},{},[525],{"type":25,"value":526},"Professor De Sensi gave us 15 minutes of it's multicore classto present our cluster to the students. This is a big opportunity to expand the plum juice team and to make the world know about us!",{"type":19,"tag":346,"props":528,"children":530},{"id":529},"whats-the-multicore-course-about",[531],{"type":25,"value":532},"What's the multicore course about?",{"type":19,"tag":28,"props":534,"children":535},{},[536,538,543,544,549,551,556,558,563,564,569],{"type":25,"value":537},"The multicore course has been tought to give a deep understanding and practical experience about parallel programming. This is done by proposing the students homeworks where they have to implement classic parallelizable problems like ",{"type":19,"tag":84,"props":539,"children":540},{},[541],{"type":25,"value":542},"matrix multiplication",{"type":25,"value":337},{"type":19,"tag":84,"props":545,"children":546},{},[547],{"type":25,"value":548},"histograms computation",{"type":25,"value":550}," and ",{"type":19,"tag":84,"props":552,"children":553},{},[554],{"type":25,"value":555},"integral approximation",{"type":25,"value":557},". The taught libraries are: ",{"type":19,"tag":84,"props":559,"children":560},{},[561],{"type":25,"value":562},"MPI, OpenMP, Pthreads",{"type":25,"value":550},{"type":19,"tag":84,"props":565,"children":566},{},[567],{"type":25,"value":568},"Cuda",{"type":25,"value":169},{"type":19,"tag":346,"props":571,"children":573},{"id":572},"what-do-we-offer",[574],{"type":25,"value":575},"What do we offer?",{"type":19,"tag":28,"props":577,"children":578},{},[579],{"type":25,"value":580},"The Plum juice team offers to give students the possibility to easily test their applications on a truly multi node system, effectively simulating more expensive clusters like Leonardo.",{"type":19,"tag":346,"props":582,"children":584},{"id":583},"our-expectations",[585],{"type":25,"value":586},"Our expectations.",{"type":19,"tag":28,"props":588,"children":589},{},[590],{"type":25,"value":591},"Every plum has their own expectactions from this presentation:",{"type":19,"tag":278,"props":593,"children":594},{},[595,605,615],{"type":19,"tag":282,"props":596,"children":597},{},[598,603],{"type":19,"tag":39,"props":599,"children":600},{},[601],{"type":25,"value":602},"Ciz:",{"type":25,"value":604}," We're dumb",{"type":19,"tag":282,"props":606,"children":607},{},[608,613],{"type":19,"tag":39,"props":609,"children":610},{},[611],{"type":25,"value":612},"Saverio:",{"type":25,"value":614}," I expect money",{"type":19,"tag":282,"props":616,"children":617},{},[618,623],{"type":19,"tag":39,"props":619,"children":620},{},[621],{"type":25,"value":622},"Lachi:",{"type":25,"value":624}," ehhh booh... nobody will be interested",{"type":19,"tag":346,"props":626,"children":628},{"id":627},"where-and-when-can-you-find-us",[629],{"type":25,"value":630},"Where and when can you find us:",{"type":19,"tag":28,"props":632,"children":633},{},[634],{"type":25,"value":635},"Aula A1, Via del Castro Laurenziano at 14:00.",{"type":19,"tag":20,"props":637,"children":639},{"id":638},"behind-the-scenes-when-nothing-was-still-working",[640],{"type":25,"value":641},"Behind the scenes, when nothing was still working",{"type":19,"tag":643,"props":644,"children":645},"blockquote",{},[646,656,665,674,684,693,702,711,721,730,739,748,754,761,770,780,782,788,798,807,817,819],{"type":19,"tag":28,"props":647,"children":648},{},[649,654],{"type":19,"tag":39,"props":650,"children":651},{},[652],{"type":25,"value":653},"Ciz",{"type":25,"value":655},": Ok, now Slurm is starting!",{"type":19,"tag":28,"props":657,"children":658},{},[659,663],{"type":19,"tag":39,"props":660,"children":661},{},[662],{"type":25,"value":653},{"type":25,"value":664},": Oh no, just kidding... it can't find the node name. Perfect, we're off to a great start.",{"type":19,"tag":28,"props":666,"children":667},{},[668,672],{"type":19,"tag":39,"props":669,"children":670},{},[671],{"type":25,"value":653},{"type":25,"value":673},": Guys, the configuration file is full of errors... honestly, it's embarrassing.",{"type":19,"tag":28,"props":675,"children":676},{},[677,682],{"type":19,"tag":39,"props":678,"children":679},{},[680],{"type":25,"value":681},"Saverio",{"type":25,"value":683},": Umm... maybe because you're running it from the master? Just a thought.",{"type":19,"tag":28,"props":685,"children":686},{},[687,691],{"type":19,"tag":39,"props":688,"children":689},{},[690],{"type":25,"value":653},{"type":25,"value":692},": Why doesn't poor ciz0 have Slurm installed? This is discrimination!",{"type":19,"tag":28,"props":694,"children":695},{},[696,700],{"type":19,"tag":39,"props":697,"children":698},{},[699],{"type":25,"value":653},{"type":25,"value":701},": Fine, I just need to add it to the playbook, right?",{"type":19,"tag":28,"props":703,"children":704},{},[705,709],{"type":19,"tag":39,"props":706,"children":707},{},[708],{"type":25,"value":653},{"type":25,"value":710},": Wait, so it's not installed at all. Great.",{"type":19,"tag":28,"props":712,"children":713},{},[714,719],{"type":19,"tag":39,"props":715,"children":716},{},[717],{"type":25,"value":718},"Ernesto",{"type":25,"value":720},": Actually, if there's no Munge, Slurm won't even acknowledge you. It's like an honor code.",{"type":19,"tag":28,"props":722,"children":723},{},[724,728],{"type":19,"tag":39,"props":725,"children":726},{},[727],{"type":25,"value":653},{"type":25,"value":729},": Ok, summary: changed the master password, removed TaskAffinity from the config because it was throwing errors anyway. Don't ask me why it's in the docs, but it doesn't work.",{"type":19,"tag":28,"props":731,"children":732},{},[733,737],{"type":19,"tag":39,"props":734,"children":735},{},[736],{"type":25,"value":653},{"type":25,"value":738},": Then, added the node name, and now Slurmd is running on all nodes! üéâ",{"type":19,"tag":28,"props":740,"children":741},{},[742,746],{"type":19,"tag":39,"props":743,"children":744},{},[745],{"type":25,"value":653},{"type":25,"value":747},": Oh, but Slurmctld won't start because it can't create /var/spool/slurmctld. Of course, it doesn't have permissions... nothing works, help me.",{"type":19,"tag":493,"props":749,"children":751},{"id":750},"the-discussion-reached-the-point-where-we-were-considering-a-full-reset",[752],{"type":25,"value":753},"... The discussion reached the point where we were considering a full reset ...",{"type":19,"tag":755,"props":756,"children":758},"h4",{"id":757},"in-the-meanwhile-saverio-had-news-that-nobody-asked-for",[759],{"type":25,"value":760},"In the meanwhile Saverio had news that nobody asked for",{"type":19,"tag":28,"props":762,"children":763},{},[764,768],{"type":19,"tag":39,"props":765,"children":766},{},[767],{"type":25,"value":681},{"type":25,"value":769},": How cool is the rack?",{"type":19,"tag":92,"props":771,"children":773},{"className":772},[230],[774,776],{"type":25,"value":775},"\n  ",{"type":19,"tag":98,"props":777,"children":779},{"src":778,"alt":8},"https://res.cloudinary.com/dllutk9zl/image/upload/v1732001405/whatsapp1_sqoiwe.jpg",[],{"type":25,"value":781},"   \n",{"type":19,"tag":755,"props":783,"children":785},{"id":784},"in-the-middle-of-desperation-the-only-friendly-answer-to-saverio-was-simones",[786],{"type":25,"value":787},"In the middle of desperation, the only friendly answer to Saverio was Simone's:",{"type":19,"tag":28,"props":789,"children":790},{},[791,796],{"type":19,"tag":39,"props":792,"children":793},{},[794],{"type":25,"value":795},"Simone",{"type":25,"value":797},": Current situation: Ciz and Ernesto desperate between Wi-Fi and Slurm.",{"type":19,"tag":28,"props":799,"children":800},{},[801,805],{"type":19,"tag":39,"props":802,"children":803},{},[804],{"type":25,"value":795},{"type":25,"value":806},": Meanwhile, Saverio in tech support mode:",{"type":19,"tag":92,"props":808,"children":810},{"className":809},[230],[811,813],{"type":25,"value":812},"\n   ",{"type":19,"tag":98,"props":814,"children":816},{"src":815,"alt":8},"https://res.cloudinary.com/dllutk9zl/image/upload/v1732001407/geppetto_muhtxu.jpg",[],{"type":25,"value":818},"  \n",{"type":19,"tag":25,"props":820,"children":823},{"className":821},[822],"centered",[824],{"type":19,"tag":755,"props":825,"children":827},{"id":826},"saverio-officially-crowned-mvp-of-confusion",[828],{"type":25,"value":829},"Saverio, officially crowned MVP of confusion.",{"type":19,"tag":20,"props":831,"children":833},{"id":832},"a-plum-at-the-sc24",[834],{"type":25,"value":835},"A Plum at the SC24",{"type":19,"tag":28,"props":837,"children":838},{},[839,841,846,848,853],{"type":25,"value":840},"This week Lorenzo attended ",{"type":19,"tag":39,"props":842,"children":843},{},[844],{"type":25,"value":845},"SC24",{"type":25,"value":847},", the world's biggest HPC conference! (what a lucky plum he is...)\nIt has been a tremendous experience and... ",{"type":19,"tag":39,"props":849,"children":850},{},[851],{"type":25,"value":852},"OH WAIT",{"type":25,"value":854},", cannot disclose it now, he will write about it!",{"type":19,"tag":28,"props":856,"children":857},{},[858],{"type":25,"value":859},"So, if you want to know what SC24 was about, stay tuned guys",{"type":19,"tag":92,"props":861,"children":862},{},[863,864,867,868],{"type":25,"value":96},{"type":19,"tag":98,"props":865,"children":866},{"src":263,"alt":8},[],{"type":25,"value":96},{"type":19,"tag":103,"props":869,"children":870},{},[871],{"type":25,"value":872},"Lorenzo at the International Conference for High Performance Computing, Networking, Storage, and Analysis",{"type":19,"tag":238,"props":874,"children":876},{":authors":875},"[{\"name\": \"Alessio Olivieri\", \"github_nickname\": \"Alessio-Olivieri\"}]",[],{"title":8,"searchDepth":243,"depth":243,"links":878},[879,883,887,894,895],{"id":268,"depth":243,"text":271,"children":880},[881],{"id":348,"depth":882,"text":351},3,{"id":430,"depth":243,"text":433,"children":884},[885,886],{"id":474,"depth":882,"text":477},{"id":501,"depth":882,"text":504},{"id":512,"depth":243,"text":515,"children":888},[889,890,891,892,893],{"id":518,"depth":882,"text":521},{"id":529,"depth":882,"text":532},{"id":572,"depth":882,"text":575},{"id":583,"depth":882,"text":586},{"id":627,"depth":882,"text":630},{"id":638,"depth":243,"text":641},{"id":832,"depth":243,"text":835},"content:blog:weekly_update_22-11.md","blog/weekly_update_22-11.md","blog/weekly_update_22-11",{"_path":900,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":901,"description":902,"author":903,"gh_nickname":904,"image":905,"date":906,"body":907,"_type":250,"_id":1214,"_source":252,"_file":1215,"_stem":1216,"_extension":255},"/blog/a_scalable_raspberry_pi_cluster","A scalable Raspberry Pi cluster","The cluster consists of eight Raspberry Pi 5 nodes: Each node is a compute node with 4GB of RAM and a quad-core ARM processor. Just one special node, with a special role, with a special personality, is a Raspberry Pi 5 with 8GB of RAM and a NVMe SSD mounted. This node is the storage node that provides a shared filesystem for the entire cluster and is used for managing other aspects of the cluster. Each node is powered through Power over Ethernet (PoE) to reduce the need for individual power supplies and simplify the cabling structure. All nodes are connected via two PoE","Francesco Fazzari","CiZ01","https://res.cloudinary.com/dllutk9zl/image/upload/v1731434779/img2_cfcjno.jpg","2024-11-11T00:00:00.000Z",{"type":16,"children":908,"toc":1205},[909,915,920,926,931,954,959,965,1010,1023,1035,1073,1078,1093,1111,1117,1149,1161,1167,1177,1183,1201],{"type":19,"tag":20,"props":910,"children":912},{"id":911},"introduction",[913],{"type":25,"value":914},"Introduction",{"type":19,"tag":28,"props":916,"children":917},{},[918],{"type":25,"value":919},"In this article, we present our project: a compact, simple and scalable cluster, composed of a set of Raspberry Pi 5 connected via Power over Ethernet (PoE) switches. The cluster is designed to be affordable and easy to manage, making it an ideal platform for educational purposes and prototyping distributed applications.",{"type":19,"tag":20,"props":921,"children":923},{"id":922},"why-use-raspberry-pi-for-clustering",[924],{"type":25,"value":925},"Why use Raspberry Pi for clustering?",{"type":19,"tag":28,"props":927,"children":928},{},[929],{"type":25,"value":930},"Raspberry Pi boards offer two main advantages for building a cluster:",{"type":19,"tag":278,"props":932,"children":933},{},[934,944],{"type":19,"tag":282,"props":935,"children":936},{},[937,942],{"type":19,"tag":39,"props":938,"children":939},{},[940],{"type":25,"value":941},"Affordability",{"type":25,"value":943},": Raspberry Pi boards are budget-friendly, for this type of project, making them accessible for educational institutions, hobbyists, and small businesses.",{"type":19,"tag":282,"props":945,"children":946},{},[947,952],{"type":19,"tag":39,"props":948,"children":949},{},[950],{"type":25,"value":951},"Scalability",{"type":25,"value":953},": Raspberry Pi clusters can be easily scaled by adding more nodes, allowing users to expand their computing power as needed or change the cluster configuration to suit different applications.",{"type":19,"tag":28,"props":955,"children":956},{},[957],{"type":25,"value":958},"Beyond these advantages, using Raspberry Pis in a cluster opens up a lot of interesting possibilities for experimenting with ARM processors. This kind of project is perfect for getting hands-on experience with distributed computing. It‚Äôs a great way to dive into how small hardware can take on real computing tasks, and it lets us play around with ideas for optimizing performance.",{"type":19,"tag":20,"props":960,"children":962},{"id":961},"hardware-setup",[963],{"type":25,"value":964},"Hardware Setup",{"type":19,"tag":28,"props":966,"children":967},{},[968,970,975,977,982,984,989,991,996,997,1002,1003,1008],{"type":25,"value":969},"The cluster consists of ",{"type":19,"tag":39,"props":971,"children":972},{},[973],{"type":25,"value":974},"eight Raspberry Pi 5 nodes",{"type":25,"value":976},":\nEach node is a compute node with ",{"type":19,"tag":39,"props":978,"children":979},{},[980],{"type":25,"value":981},"4GB of RAM",{"type":25,"value":983}," and a ",{"type":19,"tag":39,"props":985,"children":986},{},[987],{"type":25,"value":988},"quad-core ARM processor",{"type":25,"value":990},".\nJust one special node, with a special role, with a special personality, is a ",{"type":19,"tag":39,"props":992,"children":993},{},[994],{"type":25,"value":995},"Raspberry Pi 5",{"type":25,"value":406},{"type":19,"tag":39,"props":998,"children":999},{},[1000],{"type":25,"value":1001},"8GB of RAM",{"type":25,"value":983},{"type":19,"tag":39,"props":1004,"children":1005},{},[1006],{"type":25,"value":1007},"NVMe SSD",{"type":25,"value":1009}," mounted. This node is the storage node that provides a shared filesystem for the entire cluster and is used for managing other aspects of the cluster.",{"type":19,"tag":92,"props":1011,"children":1012},{},[1013,1014,1017,1018],{"type":25,"value":96},{"type":19,"tag":98,"props":1015,"children":1016},{"src":905,"alt":995},[],{"type":25,"value":96},{"type":19,"tag":103,"props":1019,"children":1020},{},[1021],{"type":25,"value":1022},"Our Raspberry Pi 5, it is a plum",{"type":19,"tag":28,"props":1024,"children":1025},{},[1026,1028,1033],{"type":25,"value":1027},"Each node is powered through ",{"type":19,"tag":39,"props":1029,"children":1030},{},[1031],{"type":25,"value":1032},"Power over Ethernet (PoE)",{"type":25,"value":1034}," this approch allow us to:",{"type":19,"tag":278,"props":1036,"children":1037},{},[1038,1061],{"type":19,"tag":282,"props":1039,"children":1040},{},[1041,1046,1048,1053,1054,1059],{"type":19,"tag":39,"props":1042,"children":1043},{},[1044],{"type":25,"value":1045},"Reduce",{"type":25,"value":1047}," the need for ",{"type":19,"tag":39,"props":1049,"children":1050},{},[1051],{"type":25,"value":1052},"individual power supplies",{"type":25,"value":550},{"type":19,"tag":39,"props":1055,"children":1056},{},[1057],{"type":25,"value":1058},"simplify the cabling structure",{"type":25,"value":1060},", this is an important aspect since each compute node requires 30W of power and a power supply that can provide enough power for a set of nodes is very expensive.",{"type":19,"tag":282,"props":1062,"children":1063},{},[1064,1066,1071],{"type":25,"value":1065},"Each node need a network connection, so we can ",{"type":19,"tag":39,"props":1067,"children":1068},{},[1069],{"type":25,"value":1070},"use the same cable for power and network connection",{"type":25,"value":1072},", ez üßé‚Äç‚ôÄÔ∏è.",{"type":19,"tag":28,"props":1074,"children":1075},{},[1076],{"type":25,"value":1077},"However the current topology is a temporary solution, we are working on improve some aspects of the network configuration, but we need money for this üí∏.",{"type":19,"tag":92,"props":1079,"children":1080},{},[1081,1082,1087,1088],{"type":25,"value":96},{"type":19,"tag":98,"props":1083,"children":1086},{"src":1084,"alt":1085},"https://res.cloudinary.com/dllutk9zl/image/upload/v1731434780/img1_ponf4t.jpg","switch",[],{"type":25,"value":96},{"type":19,"tag":103,"props":1089,"children":1090},{},[1091],{"type":25,"value":1092},"One of our powerfull switch ‚ö°",{"type":19,"tag":28,"props":1094,"children":1095},{},[1096,1098,1103,1105,1109],{"type":25,"value":1097},"Last but not least, the cluster is managed by a very strong ",{"type":19,"tag":39,"props":1099,"children":1100},{},[1101],{"type":25,"value":1102},"Raspberry Pi 3B",{"type":25,"value":1104}," acting as the master and login node. This node is responsible for coordinating tasks, managing the cluster and interacting with the compute nodes. For this kind of usage we have chosen a ",{"type":19,"tag":39,"props":1106,"children":1107},{},[1108],{"type":25,"value":1102},{"type":25,"value":1110}," because it should be enough for this kind of tasks.",{"type":19,"tag":20,"props":1112,"children":1114},{"id":1113},"software-setup",[1115],{"type":25,"value":1116},"Software setup",{"type":19,"tag":28,"props":1118,"children":1119},{},[1120,1122,1127,1129,1134,1136,1140,1142,1147],{"type":25,"value":1121},"Each node run ",{"type":19,"tag":39,"props":1123,"children":1124},{},[1125],{"type":25,"value":1126},"Raspberry Pi OS Lite 64 bits",{"type":25,"value":1128}," , with ",{"type":19,"tag":39,"props":1130,"children":1131},{},[1132],{"type":25,"value":1133},"OpenMPI",{"type":25,"value":1135}," for parallel computing tasks, currently no containerization technology is used, any tasks run in a bare metal environment.",{"type":19,"tag":1137,"props":1138,"children":1139},"br",{},[],{"type":25,"value":1141},"\nAll this hardware need someone to manage them, we choose ",{"type":19,"tag":39,"props":1143,"children":1144},{},[1145],{"type":25,"value":1146},"SLURM",{"type":25,"value":1148}," that is a free and open-source job scheduler, that can be used to manage and schedule the tasks on the cluster. It is a very powerful tool that allows us to manage the resources of the cluster in a very simple way using configuration files, however this is not the place where we can explain how we have configured it and other software that we use in the cluster, maybe in the future we will write an article about it.",{"type":19,"tag":28,"props":1150,"children":1151},{},[1152,1154,1159],{"type":25,"value":1153},"An important aspect of the software setup is the support of th ",{"type":19,"tag":39,"props":1155,"children":1156},{},[1157],{"type":25,"value":1158},"Ansible configuration management tool",{"type":25,"value":1160},", that allows us to manage the configuration of the cluster in a very simple way, we can install software, update the system, and manage the configuration of the cluster with just a command and propagate the changes to all the nodes in the cluster. This is simple to use but it is a pain to configure, however it is worth it üí™.",{"type":19,"tag":20,"props":1162,"children":1164},{"id":1163},"conclusion",[1165],{"type":25,"value":1166},"Conclusion",{"type":19,"tag":28,"props":1168,"children":1169},{},[1170,1172,1175],{"type":25,"value":1171},"Our Raspberry Pi cluster is a flexible, powerful solution that shows how versatile Raspberry Pi boards can be for distributed computing tasks.",{"type":19,"tag":1137,"props":1173,"children":1174},{},[],{"type":25,"value":1176},"\nWhile it‚Äôs not a substitute for large-scale server clusters, it‚Äôs a valuable platform for learning, prototyping, and testing distributed applications.",{"type":19,"tag":346,"props":1178,"children":1180},{"id":1179},"future-works",[1181],{"type":25,"value":1182},"Future Works",{"type":19,"tag":28,"props":1184,"children":1185},{},[1186,1188,1193,1194,1199],{"type":25,"value":1187},"We‚Äôre planning to add new nodes to the cluster (but of course, there are never enough ports on switches üò≠). We‚Äôre also considering adding new hardware to boost node computation, like ",{"type":19,"tag":39,"props":1189,"children":1190},{},[1191],{"type":25,"value":1192},"GPUs",{"type":25,"value":337},{"type":19,"tag":39,"props":1195,"children":1196},{},[1197],{"type":25,"value":1198},"TPUs",{"type":25,"value":1200},", and other accelerators. These upgrades would let us take on more complex tasks and experiment with hardware-accelerated processing, pushing our Pi cluster‚Äôs capabilities even further! üöÄ.",{"type":19,"tag":238,"props":1202,"children":1204},{":authors":1203},"[{\"name\":\"Francesco Fazzari\",\"github_nickname\":\"CiZ01\"}]",[],{"title":8,"searchDepth":243,"depth":243,"links":1206},[1207,1208,1209,1210,1211],{"id":911,"depth":243,"text":914},{"id":922,"depth":243,"text":925},{"id":961,"depth":243,"text":964},{"id":1113,"depth":243,"text":1116},{"id":1163,"depth":243,"text":1166,"children":1212},[1213],{"id":1179,"depth":882,"text":1182},"content:blog:a_scalable_raspberry_pi_cluster.md","blog/a_scalable_raspberry_pi_cluster.md","blog/a_scalable_raspberry_pi_cluster",{"_path":1218,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":1219,"description":1220,"author":1221,"gh_nickname":1222,"date":1223,"image":1224,"body":1225,"_type":250,"_id":1357,"_source":252,"_file":1358,"_stem":1359,"_extension":255},"/blog/every_hero_has_an_origin_story","Every hero has an origin story","Three computer science students attended a summer school in Trento to explore HPC topics in depth. Throughout the conference week, they shared a house, meals, and experiences, fostering a strong sense of community. Inspired by their passion for collaboration, they decided to form a group to create a small cluster for their experiments. After the conference, these three students began working together to build a team that shared their vision and love for research and social interaction. Others from diverse backgrounds and specialties joined them, united by a single goal: to create an open cluster built and managed by anyone who wants to contribute.","Lorenzo Piarulli, Simone Palmieri","lor3ny, sudo-err","04 November 2024","https://i.imgur.com/ttIxpCA.jpeg",{"type":16,"children":1226,"toc":1350},[1227,1233,1247,1253,1258,1264,1278,1299,1305,1318,1335,1341,1346],{"type":19,"tag":20,"props":1228,"children":1230},{"id":1229},"once-upon-a-time",[1231],{"type":25,"value":1232},"Once upon a time...",{"type":19,"tag":28,"props":1234,"children":1235},{},[1236,1238,1245],{"type":25,"value":1237},"Three computer science students attended a ",{"type":19,"tag":71,"props":1239,"children":1242},{"href":1240,"rel":1241,"target":77},"https://hpc-summer-school-24.disi.unitn.it/",[75,76],[1243],{"type":25,"value":1244},"summer school in Trento",{"type":25,"value":1246}," to explore HPC topics in depth. Throughout the conference week, they shared a house, meals, and experiences, fostering a strong sense of community. Inspired by their passion for collaboration, they decided to form a group to create a small cluster for their experiments. After the conference, these three students began working together to build a team that shared their vision and love for research and social interaction. Others from diverse backgrounds and specialties joined them, united by a single goal: to create an open cluster built and managed by anyone who wants to contribute.",{"type":19,"tag":20,"props":1248,"children":1250},{"id":1249},"the-challenge",[1251],{"type":25,"value":1252},"The challenge",{"type":19,"tag":28,"props":1254,"children":1255},{},[1256],{"type":25,"value":1257},"Piece by piece, raspberry by raspberry, the cluster began to take shape. The goal was to create a system that required minimal maintenance from scratch. They were (and still are, except for Ernesto) not initially equipped to build the system, so they learned along the way using a trial-and-error approach.",{"type":19,"tag":20,"props":1259,"children":1261},{"id":1260},"the-mission",[1262],{"type":25,"value":1263},"The mission",{"type":19,"tag":28,"props":1265,"children":1266},{},[1267,1269,1276],{"type":25,"value":1268},"This same trial and error is what students confronting this field often feel like not being able to ever touch. Having to build one's own mini-cluster can still be daunting, and having a handy remote access to ",{"type":19,"tag":71,"props":1270,"children":1273},{"href":1271,"rel":1272,"target":77},"https://www.hpe.com/us/en/compute/hpc/cray/oak-ridge-national-laboratory.html",[75,76],[1274],{"type":25,"value":1275},"HP Frontier's 600k CPU cores",{"type":25,"value":1277}," may be an even bigger challenge. That's our mission: giving everyone who wants to experience HPC in first person and experiment a bit with parallel computing a cheap and accessible remote lab.",{"type":19,"tag":1279,"props":1280,"children":1282},"div",{"align":1281},"center",[1283,1284,1289,1292,1293],{"type":25,"value":96},{"type":19,"tag":98,"props":1285,"children":1288},{"src":1286,"style":1287},"https://i.imgur.com/rX4ipCW.jpeg","width:60vw",[],{"type":19,"tag":1137,"props":1290,"children":1291},{},[],{"type":25,"value":96},{"type":19,"tag":84,"props":1294,"children":1296},{"style":1295},"font-size:small;",[1297],{"type":25,"value":1298},"The guy she told you not to worry about",{"type":19,"tag":20,"props":1300,"children":1302},{"id":1301},"the-rise",[1303],{"type":25,"value":1304},"The rise",{"type":19,"tag":28,"props":1306,"children":1307},{},[1308,1310,1316],{"type":25,"value":1309},"Using Linux, ",{"type":19,"tag":71,"props":1311,"children":1314},{"href":1312,"rel":1313,"target":77},"https://slurm.schedmd.com/documentation.html",[75,76],[1315],{"type":25,"value":1146},{"type":25,"value":1317}," and a lot of Ernesto's sweat, we were able to build and setup a first, minimal instance of the PlumJuice cluster, a collection of Raspberry Pi 5's ready to offer their computing time to curious students that are willing to support this small but ambitious project. It will be thanks to these students and their support that the project may, one day, become even more juicy.",{"type":19,"tag":1279,"props":1319,"children":1320},{"align":1281},[1321,1322,1326,1329,1330],{"type":25,"value":96},{"type":19,"tag":98,"props":1323,"children":1325},{"src":1224,"style":1324},"width:80vw",[],{"type":19,"tag":1137,"props":1327,"children":1328},{},[],{"type":25,"value":96},{"type":19,"tag":84,"props":1331,"children":1332},{"style":1295},[1333],{"type":25,"value":1334},"The results of Ernesto's sweat and blood",{"type":19,"tag":20,"props":1336,"children":1338},{"id":1337},"epilogue",[1339],{"type":25,"value":1340},"Epilogue",{"type":19,"tag":28,"props":1342,"children":1343},{},[1344],{"type":25,"value":1345},"Sharing is caring, and we're now ready to share the up-and-running PlumJuice cluster with whoever wants to have a sip of it. Our future goals are to organize events with other students and researchers, and to expand the horizon of HPC to new and accessible territories. Please run your optimized distributed parallel algorithms on the cluster, our raspberries will thank you.",{"type":19,"tag":238,"props":1347,"children":1349},{":authors":1348},"[{\"name\": \"Simone Palmieri\", \"github_nickname\": \"sudo-err\"}, {\"name\": \"Lorenzo Piarulli\", \"github_nickname\": \"lor3ny\"}]",[],{"title":8,"searchDepth":243,"depth":243,"links":1351},[1352,1353,1354,1355,1356],{"id":1229,"depth":243,"text":1232},{"id":1249,"depth":243,"text":1252},{"id":1260,"depth":243,"text":1263},{"id":1301,"depth":243,"text":1304},{"id":1337,"depth":243,"text":1340},"content:blog:every_hero_has_an_origin_story.md","blog/every_hero_has_an_origin_story.md","blog/every_hero_has_an_origin_story",1733743564641]